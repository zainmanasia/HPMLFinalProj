{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O3c_fULntNYV"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# device = torch.device(\"cpu\")\n",
        "device = torch.device(\"cuda\")\n",
        "def sync_time():\n",
        "  if device == torch.device(\"cuda\"):\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YZ3B_wUdTWqd"
      },
      "outputs": [],
      "source": [
        "# Resnet Model taken from open source git repo \n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=True\n",
        "        )\n",
        "        # self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            planes, planes, kernel_size=3, stride=1, padding=1, bias=True\n",
        "        )\n",
        "        # self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_planes,\n",
        "                    self.expansion * planes,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,\n",
        "                    bias=True,\n",
        "                )\n",
        "                # nn.BatchNorm2d(self.expansion * planes),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.conv1(x))\n",
        "        # out = self.bn2(self.conv2(out))\n",
        "        out = self.conv2(out)\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=True)\n",
        "        # self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def dataloader_split(data_path, num_workers):\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.4914, 0.4822, 0.4465],\n",
        "        std=[0.2023, 0.1994, 0.2010],\n",
        "    )\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Data\n",
        "    data_train = datasets.CIFAR10(\n",
        "        root=data_path, train=True, download=True, transform=transform\n",
        "    )\n",
        "    data_test = datasets.CIFAR10(\n",
        "        root=data_path, train=False, download=True, transform=transform\n",
        "    )\n",
        "    # Loaders\n",
        "    dataloader_train = DataLoader(data_train, batch_size=128, num_workers=0)\n",
        "    dataloader_test = DataLoader(data_test, batch_size=100, num_workers=0)\n",
        "    return dataloader_train, dataloader_test\n",
        "\n",
        "\n",
        "def get_optimizer(name, params, lr, momentum, weight_decay):\n",
        "    if name == \"SGD\":\n",
        "        opt_kwargs = dict(lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "        return torch.optim.SGD(params, **opt_kwargs)\n",
        "    elif name == \"SGD_NESTEROV\":\n",
        "        opt_kwargs = dict(\n",
        "            lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=True\n",
        "        )\n",
        "        return torch.optim.SGD(params, **opt_kwargs)\n",
        "    elif name == \"ADAGRAD\":\n",
        "        opt_kwargs = dict(lr=lr, weight_decay=weight_decay)\n",
        "        return torch.optim.Adagrad(params, **opt_kwargs)\n",
        "    elif name == \"ADADELTA\":\n",
        "        opt_kwargs = dict(lr=lr, weight_decay=weight_decay)\n",
        "        return torch.optim.Adadelta(params, **opt_kwargs)\n",
        "    elif name == \"ADAM\":\n",
        "        opt_kwargs = dict(lr=lr, weight_decay=weight_decay)\n",
        "        return torch.optim.Adam(params, **opt_kwargs)\n",
        "    else:\n",
        "        raise TypeError(\"Optimizer requested is not available\")\n",
        "\n",
        "\n",
        "def train_loop(use_cuda, device, model, loss_fn,\n",
        "               optimizer, data_gen, steps, data_in_memory):\n",
        "    data_loading_time = []\n",
        "    training_time = []\n",
        "    t0  = time.time()\n",
        "    i = 0\n",
        "    def _train_loop(images, labels):\n",
        "        data_loading_time.append(time.time() - t_data_loading_start)\n",
        "        # Training phase\n",
        "        t_training_start = time.time()\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        # Backwards pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        training_accurary = torch.sum(preds == labels) / len(preds)\n",
        "        sync_time()\n",
        "        training_time.append(time.time() - t_training_start)\n",
        "        return training_accurary, loss\n",
        "\n",
        "    if data_in_memory:\n",
        "      t_data_loading_start = time.time()\n",
        "      for images, labels in data_gen:\n",
        "        training_accurary, loss = _train_loop(images, labels)\n",
        "        t_data_loading_start = time.time()\n",
        "        i += 1\n",
        "        if i % 25 == 0:\n",
        "          print(\"PCT done: \", i/steps)\n",
        "    else:\n",
        "      while True:\n",
        "        # Dataloading phase\n",
        "        t_data_loading_start = time.time()\n",
        "        try:\n",
        "            images, labels = next(data_gen)\n",
        "        except StopIteration:\n",
        "            break  \n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        sync_time()\n",
        "        training_accurary, loss = _train_loop(images, labels)\n",
        "        i += 1\n",
        "        if i % 25 == 0:\n",
        "          print(\"PCT done: \", i/steps)\n",
        "        if i >= steps:\n",
        "          break    \n",
        "\n",
        "    return loss,training_accurary, sum(training_time), sum(data_loading_time), time.time() - t0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPPpWIZ755oU",
        "outputId": "9d8c0b5f-7476-49d1-b07a-8cc1c4b4fd7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "class Args():\n",
        "  data_path = \"~/\"\n",
        "  cuda = \"TRUE\"\n",
        "  num_workers = 2\n",
        "  opt = \"SGD\"\n",
        "  num_epochs = 1\n",
        "\n",
        "args = Args()\n",
        "num_workers = int(args.num_workers)\n",
        "data_path = args.data_path\n",
        "dataloader_train, dataloader_test = dataloader_split(data_path, 0)\n",
        "\n",
        "use_cuda = args.cuda\n",
        "opt_name = args.opt\n",
        "learning_rate = 0.1\n",
        "momentum = 0.9\n",
        "decay = 5e-4\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_no_opt(model, device):\n",
        "    optimizer = get_optimizer(\n",
        "      opt_name, model.parameters(), learning_rate, momentum, decay\n",
        "    )\n",
        "    total_steps = len(dataloader_train)\n",
        "    accuracy = []\n",
        "    losses = []\n",
        "    total_time = []\n",
        "    training_time = []\n",
        "    data_loading_time = []\n",
        "    \n",
        "    # Training model\n",
        "    data_gen = iter(dataloader_train)\n",
        "    loss, training_accurary, t_time, d_time, tot_time = train_loop(\n",
        "        use_cuda,\n",
        "        device,\n",
        "        model,\n",
        "        loss_fn,\n",
        "        optimizer,\n",
        "        data_gen,\n",
        "        total_steps,\n",
        "        False\n",
        "    )\n",
        "\n",
        "    # Keeping track of metrics\n",
        "    losses.append(loss.item())\n",
        "    accuracy.append(training_accurary)\n",
        "\n",
        "    # Time measurements\n",
        "    total_time.append(tot_time)\n",
        "    training_time.append(t_time)\n",
        "    data_loading_time.append(d_time)\n",
        "    \n",
        "    print(\n",
        "        f\"Total time: {sum(total_time)}\\nTraining time: {sum(training_time)} \"\n",
        "        f\"\\nData-loading time: {sum(data_loading_time)}\\n\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LMK1au1yl8pj"
      },
      "outputs": [],
      "source": [
        "def eval(model: nn.Module, dataloader: DataLoader, data_in_memory, subset=None) -> float:\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    data_time = []\n",
        "    inference_time = []\n",
        "    i = 0\n",
        "    with torch.no_grad():\n",
        "        t0 = time.time()\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            if not data_in_memory:\n",
        "              images = images.to(device)\n",
        "              labels = labels.to(device)\n",
        "            sync_time()\n",
        "            data_time.append(time.time() - t0)\n",
        "            t1 = time.time()\n",
        "            outputs = model(images)\n",
        "            sync_time()\n",
        "            inference_time.append(time.time() - t1)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            t0 = time.time()\n",
        "            i += 1\n",
        "            if subset is not None:\n",
        "                if i >= subset:\n",
        "                    break\n",
        "    print(\"Data loading took:\", sum(data_time))\n",
        "    print(\"Forward took:\", sum(inference_time))\n",
        "    return 100 * correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aRCIU4zDpWb"
      },
      "source": [
        "Profiling training of non optimized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D5DE9pi6rvt",
        "outputId": "789d0eaa-4d22-4e31-eb00-a1bc4306169a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "PCT done:  0.0639386189258312\n",
            "PCT done:  0.1278772378516624\n",
            "PCT done:  0.1918158567774936\n",
            "PCT done:  0.2557544757033248\n",
            "PCT done:  0.319693094629156\n",
            "PCT done:  0.3836317135549872\n",
            "PCT done:  0.4475703324808184\n",
            "PCT done:  0.5115089514066496\n",
            "PCT done:  0.5754475703324808\n",
            "PCT done:  0.639386189258312\n",
            "PCT done:  0.7033248081841432\n",
            "PCT done:  0.7672634271099744\n",
            "PCT done:  0.8312020460358056\n",
            "PCT done:  0.8951406649616368\n",
            "PCT done:  0.959079283887468\n",
            "Total time: 60.14378309249878\n",
            "Training time: 34.743096113204956 \n",
            "Data-loading time: 25.358655214309692\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(device)\n",
        "model = ResNet18().to(device)\n",
        "train_no_opt(model, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcSyAlFaDwl0"
      },
      "source": [
        "Profiling inference of non optimized model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u8rQHSEnSm8",
        "outputId": "9b167237-41e2-4d68-d0fa-1e05e60d77fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loading took: 5.096791982650757\n",
            "Forward took: 2.1158342361450195\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "30.67"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acc = eval(model, iter(dataloader_test), False)\n",
        "acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbyFzPrH4JUC"
      },
      "source": [
        "## 2. Performance optimizations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Dataloader optimization\n",
        "Loading data in parallel and pinning the memory for faster data transfer between CPU and GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlEbQ3GTE_Lp",
        "outputId": "3d5f8edc-0ed8-4669-a550-03e11f1b51c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "def dataloader_split_opt(data_path, num_workers, device):\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.4914, 0.4822, 0.4465],\n",
        "        std=[0.2023, 0.1994, 0.2010],\n",
        "    )\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Data\n",
        "    data_train = datasets.CIFAR10(\n",
        "        root=data_path, train=True, download=True, transform=transform\n",
        "    )\n",
        "    data_test = datasets.CIFAR10(\n",
        "        root=data_path, train=False, download=True, transform=transform\n",
        "    )\n",
        "    # Loaders\n",
        "    dataloader_train = DataLoader(data_train,\n",
        "                                  batch_size=128,\n",
        "                                  num_workers=num_workers,\n",
        "                                  pin_memory=True,\n",
        "                                  )\n",
        "    dataloader_test = DataLoader(data_test,\n",
        "                                 batch_size=100,\n",
        "                                 num_workers=num_workers,\n",
        "                                 pin_memory=True,\n",
        "                                 )\n",
        "    return dataloader_train, dataloader_test\n",
        "\n",
        "data_train_opt, data_test_opt = dataloader_split_opt(data_path, 4, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rjMhXIzzEGtu"
      },
      "outputs": [],
      "source": [
        "def train_dataloader_optimized(model, device, data):\n",
        "    optimizer = get_optimizer(\n",
        "      opt_name, model.parameters(), learning_rate, momentum, decay\n",
        "    )\n",
        "    total_steps = 391\n",
        "    accuracy = []\n",
        "    losses = []\n",
        "    total_time = []\n",
        "    training_time = []\n",
        "    data_loading_time = []\n",
        "    \n",
        "    # Training model\n",
        "    loss, training_accurary, t_time, d_time, tot_time = train_loop(\n",
        "        use_cuda,\n",
        "        device,\n",
        "        model,\n",
        "        loss_fn,\n",
        "        optimizer,\n",
        "        iter(data),\n",
        "        total_steps,\n",
        "        False\n",
        "    )\n",
        "\n",
        "    # Keeping track of metrics\n",
        "    losses.append(loss.item())\n",
        "    accuracy.append(training_accurary)\n",
        "\n",
        "    # Time measurements\n",
        "    total_time.append(tot_time)\n",
        "    training_time.append(t_time)\n",
        "    data_loading_time.append(d_time)\n",
        "    \n",
        "    print(\n",
        "        f\"Total time: {sum(total_time)}\\nTraining time: {sum(training_time)} \"\n",
        "        f\"\\nData-loading time: {sum(data_loading_time)}\\n\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "o5W0G5bPD7Qx",
        "outputId": "ba26a5b6-4090-4a06-8119-d3588c8bd0ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "PCT done:  0.0639386189258312\n",
            "PCT done:  0.1278772378516624\n",
            "PCT done:  0.1918158567774936\n",
            "PCT done:  0.2557544757033248\n",
            "PCT done:  0.319693094629156\n",
            "PCT done:  0.3836317135549872\n",
            "PCT done:  0.4475703324808184\n",
            "PCT done:  0.5115089514066496\n",
            "PCT done:  0.5754475703324808\n",
            "PCT done:  0.639386189258312\n",
            "PCT done:  0.7033248081841432\n",
            "PCT done:  0.7672634271099744\n",
            "PCT done:  0.8312020460358056\n",
            "PCT done:  0.8951406649616368\n",
            "PCT done:  0.959079283887468\n",
            "Total time: 34.125473737716675\n",
            "Training time: 33.58079671859741 \n",
            "Data-loading time: 0.4668581485748291\n",
            "\n",
            "Data loading took: 0.15177607536315918\n",
            "Forward took: 2.1219701766967773\n",
            "acc: 30.82%\n"
          ]
        }
      ],
      "source": [
        "print(device)\n",
        "opt_model = ResNet18().to(device)\n",
        "train_dataloader_optimized(opt_model, device, data_train_opt)\n",
        "acc = eval(model, iter(data_test_opt), False)\n",
        "print(f\"acc: {acc}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ODylAic64PUL"
      },
      "source": [
        "## 2.2 Torch.jit.script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Qkk1gBsD4Ii2"
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.benchmark = True\n",
        "# torch.jit.enable_onednn_fusion(True)\n",
        "sample_data, _ = next(iter(dataloader_train))\n",
        "sample_data = sample_data.to(device)\n",
        "opt_model.eval()\n",
        "opt_model_traced = torch.jit.trace(opt_model, sample_data)\n",
        "opt_model_traced = torch.jit.freeze(opt_model_traced)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iID2JGYU4Ism"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loading took: 0.08617377281188965\n",
            "Forward took: 0.055468082427978516\n",
            "Data loading took: 0.08517956733703613\n",
            "Forward took: 2.3111395835876465\n",
            "Data loading took: 0.07864594459533691\n",
            "Forward took: 0.02542567253112793\n",
            "Data loading took: 0.08821630477905273\n",
            "Forward took: 0.02588343620300293\n",
            "Data loading took: 0.08326125144958496\n",
            "Forward took: 0.024461984634399414\n",
            "Data loading took: 0.08291435241699219\n",
            "Forward took: 0.024697065353393555\n",
            "Data loading took: 0.0662088394165039\n",
            "Forward took: 0.02559661865234375\n",
            "Data loading took: 0.07740545272827148\n",
            "Forward took: 0.025318622589111328\n",
            "Data loading took: 0.07469415664672852\n",
            "Forward took: 0.02451491355895996\n",
            "Data loading took: 0.07565593719482422\n",
            "Forward took: 0.02480483055114746\n",
            "Data loading took: 0.1486215591430664\n",
            "Forward took: 2.0996038913726807\n",
            "acc: 35.28%\n"
          ]
        }
      ],
      "source": [
        "warmup_iters = 10\n",
        "for i in range(warmup_iters):\n",
        "    eval(opt_model_traced, iter(data_test_opt), False)\n",
        "\n",
        "acc = eval(opt_model_traced, iter(data_test_opt), False)\n",
        "print(f\"acc: {acc}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "opt_model.eval()\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    opt_model, {torch.nn.Linear, torch.nn.Conv2d}, dtype=torch.qint8\n",
        ")\n",
        "quantized_model_traced = torch.jit.trace(quantized_model, sample_data)\n",
        "quantized_model_traced = torch.jit.freeze(quantized_model_traced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "NotImplementedError",
          "evalue": "Could not run 'quantized::linear_dynamic' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear_dynamic' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at ../aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp:656 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:35 [backend fallback]\nAutogradCPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:39 [backend fallback]\nAutogradCUDA: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:47 [backend fallback]\nAutogradXLA: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:51 [backend fallback]\nAutogradMPS: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:59 [backend fallback]\nAutogradXPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:43 [backend fallback]\nAutogradHPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:68 [backend fallback]\nAutogradLazy: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:55 [backend fallback]\nTracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:296 [backend fallback]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/var/tmp/ipykernel_13415/1341149804.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwarmup_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarmup_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantized_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantized_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/tmp/ipykernel_13415/2398665613.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(model, dataloader, data_in_memory, subset)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mdata_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0msync_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0minference_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/tmp/ipykernel_13415/195911727.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/ao/nn/quantized/dynamic/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 Y = torch.ops.quantized.linear_dynamic(\n\u001b[0;32m---> 50\u001b[0;31m                     x, self._packed_params._packed_params, reduce_range=True)\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_packed_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             Y = torch.ops.quantized.linear_dynamic_fp16(\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# We save the function ptr as the `op` attribute on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;31m# OpOverloadPacket to access it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'quantized::linear_dynamic' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear_dynamic' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at ../aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp:656 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:35 [backend fallback]\nAutogradCPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:39 [backend fallback]\nAutogradCUDA: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:47 [backend fallback]\nAutogradXLA: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:51 [backend fallback]\nAutogradMPS: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:59 [backend fallback]\nAutogradXPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:43 [backend fallback]\nAutogradHPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:68 [backend fallback]\nAutogradLazy: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:55 [backend fallback]\nTracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:296 [backend fallback]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
          ]
        }
      ],
      "source": [
        "warmup_iters = 5\n",
        "for i in range(warmup_iters):\n",
        "    eval(quantized_model_traced, iter(data_test_opt), False)\n",
        "\n",
        "acc = eval(quantized_model_traced, iter(data_test_opt), False)\n",
        "print(f\"acc: {acc}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
